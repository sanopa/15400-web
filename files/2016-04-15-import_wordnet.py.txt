import sys, re, collections
from nltk.parse.stanford import StanfordDependencyParser, StanfordNeuralDependencyParser
from nltk.corpus import wordnet as wn
from pattern.en import conjugate, singularize

# removing underscores from the concept names to be consistent with Scone
def remove_underscores(word):
  words = word.split("_")
  return " ".join(words)

def discover_sense_for_name(def_name, synset_name):
  original_synset = wn.synset(synset_name)
  possible_synsets = wn.synsets(def_name, pos='n')
  if len(possible_synsets) == 0:
    # if this word is not in WordNet, it cannot be associated with a sense
    return ''

  if len(possible_synsets) == 1:
    return possible_synsets[0].name()

  hyp_rel = lambda s:s.hypernyms()
  path_to_synset = list(original_synset.closure(hyp))
  potential_hypernyms = [s for s in possible_synsets if s in path_to_synset]
  if len(potential_hypernyms) == 1 and len(potential_hypernyms[0].hypernyms()) > 0:
    return (potential_hypernyms[0].name(), potential_hypernyms[0].hypernyms()[0].name())
  elif len(potential_hypernyms) == 1:
    return (potential_hypernyms[0].name(), '')
  elif len(potential_hypernyms) > 1:
    potential_hypernyms_idx = [path_to_synset.index(e) for e in potential_hypernyms]
    potential_hypernyms_idx.sort()
    return (path_to_synset[potential_hypernyms_idx[0]].name(),
            path_to_synset[potential_hypernyms_idx[0] + 1].name()
            if potential_hypernyms_idx[0] + 1 < len(path_to_synset) else "{thing}")
  else:
    # the noun does not occur in the hypernym list -> try to find the shortest
    # path between all the possible senses
    # TODO: implement; for now, return '' if none of the senses (or more than one)
    # are present in the potential list
    return ('', '')


scone_import_file = sys.argv[1]
scone_file = open(scone_import_file, 'a')
path_to_jar = 'stanford-corenlp/stanford-corenlp.jar'
path_to_models_jar = 'stanford-corenlp/stanford-corenlp-3.5.2-models.jar'
dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar,
                                             path_to_models_jar = path_to_models_jar)
neural_parser = StanfordNeuralDependencyParser(path_to_jar=path_to_jar,
                                               path_to_models_jar = path_to_models_jar)

# "entity" is the highest node out of all WordNet nouns
root = wn.synset('entity.n.01')

# DFS through the WordNet hyper/hyponym graph
visited = set()
visited.add(root)
scone_file.write("(new-wordnet-type {" + root.name() + "} {thing})\n")
typ, inst = 't', 'i'
stack = [(r, root, typ) for r in root.hyponyms()]
# holonyms will be added at the end
# each list element will be (x, y, t) where x is a holonym of y and t is either
# a part, substance, or member type
part, substance, member = 'p', 's', 'm'
holonyms = []
intersection_types = []
appos_types = []
relations = []
parent = root
roles = []
while len(stack) > 0:
  current, parent, tp = stack.pop()
  if (current, parent, tp) not in visited:
    visited.add((current, parent, tp))
    if tp == typ:
      scone_file.write("(new-wordnet-type {")
    else:
      scone_file.write("(new-wordnet-indv {")
    scone_file.write(current.name() + "} {" + parent.name() + "})\n")
    scone_file.write("(english {" + current.name() + "}")
    for lemma in current.lemmas():
      scone_file.write(" \"" + remove_underscores(lemma.name()) + "\"")
    scone_file.write(")\n")
    for instance in current.instance_hyponyms():
      stack.append((instance, current, inst))
    for hyp in current.hyponyms():
      stack.append((hyp, current, typ))
    for h in current.part_holonyms():
      holonyms.append((h, current, part))
    for h in current.substance_holonyms():
      holonyms.append((h, current, substance))
    for h in current.member_holonyms():
      holonyms.append((h, current, member))

    # if the definition is composed of several "sentences", take the first one
    # for now
    defn = current.definition()
    split_by_semi = defn.split("; ")
    if len(split_by_semi) > 1:
      defn = split_by_semi[0]

    # parse the definition
    parse = dependency_parser.raw_parse(defn).next()
    nparse = neural_parser.raw_parse(defn).next()
    definition_triples = list(parse.triples())
    ndefinition_triples = list(nparse.triples())

    # the first pair in the dependency triples is the top of the tree; ~ matches
    # the actual hypernym, so use relations with that word to add information
    # to Scone
    defn_hypernym, defn_hypernym_pos = definition_triples[0][0]
    ndefn_hypernym, ndefn_hypernym_pos = ndefinition_triples[0][0]
    # neural network-based & regular parser results don't match
    parse_tree = None
    if defn_hypernym == ndefn_hypernym and defn_hypernym == parent.name():
      parse_tree = nparse
    elif defn_hypernym == parent.name():
      parse_tree = parse
    elif ndefn_hypernym == parent.name():
      parse_tree = nparse
    elif defn_hypernym_pos[0] == 'N' and ndefn_hypernym_pos[0] == 'N':
      continue
    elif defn_hypernym_pos[0] == 'N':
      parse_tree = parse
    elif ndefn_hypernym_pos[0] == 'N':
      parse_tree = nparse
    else:
      continue

    name = parse_tree.root['word']
    if 'compound' in parse_tree.root['deps']:
      # for now, ignore if there are several compounds
      compound = parse_tree.root['deps']['compound'][0]
      name = parse_tree.nodes[compound]['word'] + "_" + name

    if 'nmod' in parse_tree.root['deps']:
      nmods = parse_tree.root['deps']['nmod']
      for idx in nmods:
        if 'case' in parse_tree.nodes[idx]['deps'] and \
        parse_tree.nodes[parse_tree.nodes[idx]['deps']['case'][0]]['word'] == 'of':
          w = parse_tree.nodes[idx]['word']
          if 'compound' in parse_tree.nodes[idx]['deps']:
          # for now, ignore if there are several compounds
            compound = parse_tree.nodes[idx]['deps']['compound'][0]
            w = parse_tree.nodes[compound]['word'] + "_" + w
          roles.append((current.name(), name, w))


    # find the modifier relations and create an intersection type
    if 'amod' in parse_tree.root['deps']:
      adjectives = parse_tree.root['deps']['amod']
      adjectives = [idx for idx in adjectives if parse_tree.nodes[idx]['ctag'] != 'JJR' \
                                              and parse_tree.nodes[idx]['ctag'] != 'JJS']
      new_types = []
      for idx in adjectives:
        # adjectives have to be {thing}s
        scone_file.write("(new-wordnet-type {" +  parse_tree.nodes[idx]['word'] + "} {thing} '(:adj))\n")
        new_types.append("{" +  parse_tree.nodes[idx]['word'] + "}")

        if 'conj' in parse_tree.nodes[idx]['deps']:
          for idy in parse_tree.nodes[idx]['deps']['conj']:
            scone_file.write("(new-wordnet-type {" +  parse_tree.nodes[idy]['word'] + "} {thing} '(:adj))\n")
            new_types.append("{" +  parse_tree.nodes[idy]['word'] + "}")
      # once the entire wordnet is added, attempt to find the most reasonable
      # sense for the main definition noun, otherwise assign to Scone knowledge
      intersection_types.append((current.name, name, new_types))

    if 'appos' in parse_tree.root['deps']:
      equivalents = parse_tree.root['deps']['appos']
      equivalents_words = []
      for idx in equivalents:
        w = parse_tree.nodes[idx]['word']
        if 'compound' in parse_tree.nodes[idx]['deps']:
          # for now, ignore if there are several compounds
          compound = parse_tree.nodes[idx]['deps']['compound'][0]
          w = parse_tree.nodes[compound]['word'] + "_" + w
        equivalents_words.append(w)
      appos_types.append((current.name(), equivalents_words))

    # pulling out relations
    if 'acl' in parse_tree.root['deps']:
      acls = parse_tree.root['deps']['acl']
      for idx in acls:
        # make sure the relation is a verb
        if parse_tree.nodes[idx]['ctag'][0] == 'V':
          relation = parse_tree.nodes[idx]['word']
          conjrelation = conjugate(relation, tense='present', person=3, number='singular',
                               mood='indicative', aspect='imperfective', negated=False)

          compounds = []
          case_prep = ''
          nmods = []
          # find the compound that the relation uses, e.g. "consists of ___"
          if 'nmod' in parse_tree.nodes[idx]['deps']:
            nmods.extend(parse_tree.nodes[idx]['deps']['nmod'])
          if 'dobj' in parse_tree.nodes[idx]['deps']:
            nmods.extend(parse_tree.nodes[idx]['deps']['dobj'])

          if len(nmods) > 0:
            for idy in nmods:
              w = parse_tree.nodes[idy]['word']
              if 'compound' in parse_tree.nodes[idy]['deps']:
                # for now, ignore if there are several compounds
                compound = parse_tree.nodes[idy]['deps']['compound'][0]
                w = parse_tree.nodes[compound]['word'] + "_" + w
              compounds.append(w)
              if 'case' in parse_tree.nodes[idy]['deps']:
                # there shouldn't be several cases on a noun
                case_prep = " " + parse_tree.nodes[parse_tree.nodes[idy]['deps']['case'][0]]['word']
          else:
            print "found acl with no nmod dependencies: ", current.name(), ":", list(parse_tree.triples())

          conjrelation = conjrelation + case_prep
          scone_file.write("(new-wordnet-relation {" + conjrelation + "})")
          relations.append((current.name(), conjrelation, compounds))

      # for ((p, ppos), rel, (c, cpos)) in definition_triples:
      #   if rel == 'conj' and (p, ppos) in adj:
      #     print current.name(), p, "conj", c


for (x, y, t) in holonyms:
  relation = ''
  if t == part:
    relation = '{part of}'
  elif t == member:
    relation = '{member of}'
  else:
    relation = '{makes up}'
  scone_file.write("(new-statement {" + y.name() + "} " + relation + " {" + x.name() + "})\n")

for (synset_name, def_name, adjectives) in intersection_types:
  wn_synset, parent = discover_sense_for_name(def_name, synset_name)
  if wn_synset == '':
    scone_file.write("(new-wordnet-type {" + def_name + "} {thing})\n")
  # begin intersection type
  scone_file.write("(new-is-a {" + synset_name + "} ")
  scone_file.write("(new-intersection-type nil '(")
  for t in adjectives:
    scone_file.write(t + " ")
  scone_file.write("{" + (wn_synset if wn_synset != '' else def_name) + "})))\n")

for (synset_name, appos) in appos_types:
  for a in appos_types:
    wn_synset, parent = discover_sense_for_name(a, synset_name)
    if wn_synset == '':
      scone_file.write("(new-wordnet-type {" + a + "} {thing})\n")
    scone_file.write("(new-is-eq {" + synset_name + "} {" + \
     (wn_synset if wn_synset != '' else a) + "})\n")

for (name, rel, compounds) in relations:
  for c in compounds:
    c_synsets = wn.synsets(c)
    c_name = c
    if len(c_synsets) == 0:
      # choose the first sense
      scone_file.write("(new-wordnet-type {" + c + "} {thing})\n")
    else:
      c_name = c_synsets[0].name()
    scone_file.write("(new-statement {" + name + "} " + rel + " {" + c_name + "})\n")

for (synset_name, role_name, dep_name) in roles:
  (wn_synset, wn_synset_parent) = discover_sense_for_name(role_name, synset_name)
  (dep_name_wn, dep_name_parent) = discover_sense_for_name(dep_name, synset_name)
  role = ''
  if wn_synset == '':
    scone_file.write("(new-type-role {" + role_name + "} {thing} {thing})\n")
    role = role_name
  elif wn_synset_parent == '':
    scone_file.write("(new-type-role {" + wn_synset + "} {thing} {thing})\n")
    role = wn_synset
  else:
    scone_file.write("(new-type-role {" + wn_synset + "} {thing} {" + wn_synset_parent + "})\n")
    role = wn_synset

  if dep_name_wn == '':
    scone_file.write("(new-wordnet-type {" + dep_name + "} {thing})\n")

  scone_file.write("(x-is-a-y-of-z {" + synset_name + "} {" + role + "} {" + (dep_name if dep_name_wn == '' else dep_name_wn) + "})\n")

scone_file.close()